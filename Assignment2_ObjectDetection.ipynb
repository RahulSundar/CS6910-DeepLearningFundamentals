{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_ObjectDetection_unbalanced_sweep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDRgbOz4FNx5wecjQuOBmU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahulSundar/CS6910-DeepLearningFundamentals/blob/main/Assignment2_ObjectDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zGZzbEmQ48A"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTTrz8WoTNlV"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JMG1uDaRGZr"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrHj1bbxHjVi"
      },
      "source": [
        "!pip install wandb\n",
        "\n",
        "!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
        "!unzip nature_12K.zip\n",
        "!rm nature_12K.zip\n",
        "\n",
        "!mv ./inaturalist_12K/val ./inaturalist_12K/test\n",
        "\n",
        "#Commented out IPython magic to ensure Python compatibility.\n",
        "%mkdir ./inaturalist_12K/eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLWAAAf2HjNg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vfj7aZ0HjGs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcIyA3VPHi6_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cykm-daIJXW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oqxnJgOIJO8"
      },
      "source": [
        "import os, sys\n",
        "sys.path.append(\n",
        "\"/content/drive/MyDrive/CS6910/Assignment2\"\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cGzCjsKIJAl"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52aBib4BNSiC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPECnwAsHjne"
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#import keras\n",
        "\n",
        "# keras pre-trained models\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2 as IRV2\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input, InputLayer, Flatten, Conv2D, BatchNormalization, MaxPooling2D, Activation \n",
        "from tensorflow.keras.models import Sequential,  Model\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "class ObjectDetection():\n",
        "\n",
        "    def __init__(self, IMG_SIZE, modelConfigDict, using_pretrained_model = False, base_model = \"IRV2\" ):\n",
        "        \n",
        "        self.num_hidden_cnn_layers= modelConfigDict[\"num_hidden_cnn_layers\"]\n",
        "        self.activation = modelConfigDict[\"activation\"]\n",
        "        self.batch_normalization = modelConfigDict[\"batch_normalization\"]\n",
        "        self.filter_distribution = modelConfigDict[\"filter_distribution\"]\n",
        "        self.filter_size = modelConfigDict[\"filter_size\"]\n",
        "        self.number_of_filters_base  = modelConfigDict[\"number_of_filters_base\"]\n",
        "        self.initializer = modelConfigDict[\"initializer\"]\n",
        "        self.dropout_fraction = modelConfigDict[\"dropout_fraction\"]\n",
        "        self.pool_size = modelConfigDict[\"pool_size\"]\n",
        "        self.padding = modelConfigDict[\"padding\"]\n",
        "        self.dense_neurons = modelConfigDict[\"dense_neurons\"]\n",
        "        self.num_classes = modelConfigDict[\"num_classes\"]\n",
        "        self.optimizer = modelConfigDict[\"optimizer\"]\n",
        "\n",
        "        BASE_MODELS = {\n",
        "                          \"IRV2\" : IRV2,\n",
        "                          \"IV3\" : InceptionV3,\n",
        "                          \"RN50\" : ResNet50,\n",
        "                          \"XCPTN\" : Xception\n",
        "                      }      \n",
        "        \n",
        "        if using_pretrained_model == True:\n",
        "            self.base_model = base_model\n",
        "            if self.base_model == \"RN50\":\n",
        "                self.IMG_HEIGHT = 224\n",
        "                self.IMG_WIDTH = 224\n",
        "            else:\n",
        "                self.IMG_HEIGHT = IMG_SIZE[0]\n",
        "                self.IMG_WIDTH = IMG_SIZE[1]        \n",
        "\n",
        "        self.IMG_HEIGHT = IMG_SIZE[0]\n",
        "        self.IMG_WIDTH = IMG_SIZE[1]        \n",
        "         \n",
        "        self.input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)\n",
        "\n",
        "\n",
        "    def build_cnndropmodel(self):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            \n",
        "            #First CNN layer connecting to input layer\n",
        "            model.add(Conv2D(self.number_of_filters_base, self.filter_size, kernel_regularizer='l2',padding = self.padding, input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)))\n",
        "            model.add(Activation(self.activation))\n",
        "            \n",
        "            #batch_normalisation\n",
        "            if self.batch_normalization: model.add(BatchNormalization())\n",
        "            #max pooling\n",
        "            model.add(MaxPooling2D(pool_size=self.pool_size))  \n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            for i in range(self.num_hidden_cnn_layers-1):\n",
        "                #i+2th Convolutional Layer\n",
        "            \n",
        "                ## Standard filter distribution - same number of filters in all Convolutional layers\n",
        "                if self.filter_distribution == \"standard\":\n",
        "                    model.add(Conv2D(self.number_of_filters_base, self.filter_size,kernel_regularizer='l2', padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                ## Double filter distribution - double number of filters in each Convolutional layers\n",
        "                elif self.filter_distribution == \"double\":\n",
        "                    model.add(Conv2D(2**(i+1)*self.number_of_filters_base, self.filter_size, kernel_regularizer='l2',padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                ## Halve the filter size in each successive convolutional layers\n",
        "                elif self.filter_distribution == \"half\":\n",
        "                    model.add(Conv2D(int(self.number_of_filters_base/2**(i+1)), self.filter_size, kernel_regularizer='l2',padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                model.add(Activation(self.activation))\n",
        "            \n",
        "                if self.batch_normalization: model.add(BatchNormalization())\n",
        "            \n",
        "                model.add(MaxPooling2D(pool_size=self.pool_size))\n",
        "                if self.dropout_fraction != None:\n",
        "                    model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            \n",
        "            #Final densely connected layers\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(self.dense_neurons, activation = self.activation, kernel_regularizer='l2', kernel_initializer = self.initializer))\n",
        "            model.add(Dense(self.num_classes, activation = 'softmax'))\n",
        "            \n",
        "            #model.compile(optimizer=self.optimizer,\n",
        "            #      loss='categorical_crossentropy',\n",
        "            #      metrics=['accuracy'])\n",
        "            return model      \n",
        "        \n",
        "    def build_cnndropmodel2(self):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            \n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            \n",
        "            #First CNN layer connecting to input layer\n",
        "            model.add(Conv2D(self.number_of_filters_base, self.filter_size, kernel_regularizer='l2',padding = self.padding, input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)))\n",
        "            model.add(Activation(self.activation))\n",
        "            \n",
        "            #batch_normalisation\n",
        "            if self.batch_normalization: model.add(BatchNormalization())\n",
        "            #max pooling\n",
        "            model.add(MaxPooling2D(pool_size=self.pool_size))  \n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            for i in range(self.num_hidden_cnn_layers-1):\n",
        "                #i+2th Convolutional Layer\n",
        "            \n",
        "                ## Standard filter distribution - same number of filters in all Convolutional layers\n",
        "                if self.filter_distribution == \"standard\":\n",
        "                    model.add(Conv2D(self.number_of_filters_base, self.filter_size,kernel_regularizer='l2', padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                ## Double filter distribution - double number of filters in each Convolutional layers\n",
        "                elif self.filter_distribution == \"double\":\n",
        "                    model.add(Conv2D(2**(i+1)*self.number_of_filters_base, self.filter_size, kernel_regularizer='l2',padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                ## Halve the filter size in each successive convolutional layers\n",
        "                elif self.filter_distribution == \"half\":\n",
        "                    model.add(Conv2D(int(self.number_of_filters_base/2**(i+1)), self.filter_size, kernel_regularizer='l2',padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                model.add(Activation(self.activation))\n",
        "            \n",
        "                if self.batch_normalization: model.add(BatchNormalization())\n",
        "            \n",
        "                model.add(MaxPooling2D(pool_size=self.pool_size))\n",
        "                if self.dropout_fraction != None:\n",
        "                    model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            \n",
        "            #Final densely connected layers\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(self.dense_neurons, activation = self.activation, kernel_regularizer='l2', kernel_initializer = self.initializer))\n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            model.add(Dense(self.num_classes, activation = 'softmax'))\n",
        "            \n",
        "            #model.compile(optimizer=self.optimizer,\n",
        "            #      loss='categorical_crossentropy',\n",
        "            #      metrics=['accuracy'])\n",
        "            return model      \n",
        "      \n",
        "    def build_cnnmodelsimple(self):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            \n",
        "            #First CNN layer connecting to input layer\n",
        "            model.add(Conv2D(self.number_of_filters_base, self.filter_size, input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)))\n",
        "            model.add(Activation(self.activation))\n",
        "            \n",
        "            #batch_normalisation\n",
        "            if self.batch_normalization: model.add(BatchNormalization())\n",
        "            #max pooling\n",
        "            model.add(MaxPooling2D(pool_size=self.pool_size))  \n",
        "            for i in range(self.num_hidden_cnn_layers-1):\n",
        "                #i+2th Convolutional Layer\n",
        "            \n",
        "                ## Standard filter distribution - same number of filters in all Convolutional layers\n",
        "                if self.filter_distribution == \"standard\":\n",
        "                    model.add(Conv2D(self.number_of_filters_base, self.filter_size))\n",
        "            \n",
        "                ## Double filter distribution - double number of filters in each Convolutional layers\n",
        "                elif self.filter_distribution == \"double\":\n",
        "                    model.add(Conv2D(2**(i+1)*self.number_of_filters_base, self.filter_size))\n",
        "            \n",
        "                ## Halve the filter size in each successive convolutional layers\n",
        "                elif self.filter_distribution == \"half\":\n",
        "                    model.add(Conv2D(int(self.number_of_filters_base/2**(i+1)), self.filter_size))\n",
        "            \n",
        "                model.add(Activation(self.activation))\n",
        "            \n",
        "                if self.batch_normalization: model.add(BatchNormalization())\n",
        "            \n",
        "                model.add(MaxPooling2D(pool_size=self.pool_size))\n",
        "            \n",
        "            #Final densely connected layers\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(self.dense_neurons, activation = 'sigmoid'))\n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            model.add(Dense(self.num_classes, activation = 'softmax'))\n",
        "            \n",
        "            #model.compile(optimizer=self.optimizer,\n",
        "            #      loss='categorical_crossentropy',\n",
        "            #      metrics=['accuracy'])\n",
        "            return model \n",
        "        \n",
        "    def build_cnnmodel(self):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            \n",
        "            #First CNN layer connecting to input layer\n",
        "            model.add(Conv2D(self.number_of_filters_base, self.filter_size, kernel_regularizer='l2',padding = self.padding, input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)))\n",
        "            model.add(Activation(self.activation))\n",
        "            \n",
        "            #batch_normalisation\n",
        "            if self.batch_normalization: model.add(BatchNormalization())\n",
        "            #max pooling\n",
        "            model.add(MaxPooling2D(pool_size=self.pool_size))  \n",
        "            for i in range(self.num_hidden_cnn_layers-1):\n",
        "                #i+2th Convolutional Layer\n",
        "            \n",
        "                ## Standard filter distribution - same number of filters in all Convolutional layers\n",
        "                if self.filter_distribution == \"standard\":\n",
        "                    model.add(Conv2D(self.number_of_filters_base, self.filter_size,kernel_regularizer='l2', padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                ## Double filter distribution - double number of filters in each Convolutional layers\n",
        "                elif self.filter_distribution == \"double\":\n",
        "                    model.add(Conv2D(2**(i+1)*self.number_of_filters_base, self.filter_size, kernel_regularizer='l2',padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                ## Halve the filter size in each successive convolutional layers\n",
        "                elif self.filter_distribution == \"half\":\n",
        "                    model.add(Conv2D(int(self.number_of_filters_base/2**(i+1)), self.filter_size, kernel_regularizer='l2',padding = self.padding, kernel_initializer = self.initializer))\n",
        "            \n",
        "                model.add(Activation(self.activation))\n",
        "            \n",
        "                if self.batch_normalization: model.add(BatchNormalization())\n",
        "            \n",
        "                model.add(MaxPooling2D(pool_size=self.pool_size))\n",
        "            \n",
        "            #Final densely connected layers\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(self.dense_neurons, activation = 'sigmoid', kernel_regularizer='l2'))\n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            model.add(Dense(self.num_classes, activation = 'softmax'))\n",
        "            \n",
        "            #model.compile(optimizer=self.optimizer,\n",
        "            #      loss='categorical_crossentropy',\n",
        "            #      metrics=['accuracy'])\n",
        "            return model      \n",
        "            \n",
        "        \n",
        "    def load_pretrained_model(self):\n",
        "        base_model = BASE_MODELS[self.base_model_name]\n",
        "        base = base_model(weights='imagenet', include_top=False)\n",
        "        x = base.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(self.dense_neurons, activation='relu')(x)\n",
        "        guesses = Dense(self.num_classes, activation='softmax')(x)\n",
        "        model = Model(inputs=base.input, outputs=guesses)\n",
        "\n",
        "        # freeze all base layers\n",
        "        for layer in base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        #model.compile(optimizer=self.optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og5Ds0aQIHWI"
      },
      "source": [
        "# data preprocessing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import pathlib\n",
        "\n",
        "#wandb logging\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "\n",
        "#physical_devices = tf.config.list_physical_devices('GPU')\n",
        "#try:\n",
        "#  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "#except:\n",
        " # Invalid device or cannot modify virtual devices once initialized.\n",
        "#  pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data pre processing\n",
        "\n",
        "data_augmentation = False\n",
        "\n",
        "IMG_SIZE = (128,128)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "if data_augmentation == True:\n",
        "\n",
        "#Faster Alternative\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            validation_split = 0.1,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False\n",
        "            )\n",
        "else:\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,validation_split = 0.1)\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    './inaturalist_12K/train',\n",
        "    subset='training',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle = True,\n",
        "     seed = 123)\n",
        "    \n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        './inaturalist_12K/train',\n",
        "        target_size=IMG_SIZE,\n",
        "        subset = 'validation',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        shuffle = True,\n",
        "         seed = 123)\n",
        "\n",
        "\n",
        "        \n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        './inaturalist_12K/test',\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        shuffle = True,\n",
        "         seed = 123)\n",
        "\n",
        "''' \n",
        "#sweep config\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"validationaccuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  'early_terminate': {\n",
        "        'type':'hyperband',\n",
        "        'min_iter': [3],\n",
        "        's': [2]\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \n",
        "        \"activation\":{\n",
        "            \"values\": [\"relu\", \"elu\"]\n",
        "        },\n",
        "                    \n",
        "        \"batch_size\": {\n",
        "            \"values\": [32, 64]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\", \"adam\", \"rmsprop\"]\n",
        "        },\n",
        "        \"batch_normalization\": {\n",
        "            \"values\": [True, False]\n",
        "        },\n",
        "        \"number_of_filters_base\": {\n",
        "            \"values\": [32, 64]\n",
        "        },\n",
        "        \"dense_neurons\": {\n",
        "            \"values\": [32, 64]\n",
        "        },\n",
        "        \"dropout_fraction\": {\n",
        "            \"values\": [0.2,0.3]\n",
        "        },        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='CS6910-DeepLearningFundamentals-Assignment1', entity='rahulsundar')\n",
        "\n",
        "'''\n",
        "\n",
        "#train function\n",
        "def train():\n",
        "\n",
        "        \n",
        "    config_defaults = dict(\n",
        "            num_hidden_cnn_layers = 5 ,\n",
        "            activation = 'relu',\n",
        "            batch_normalization = True,\n",
        "            filter_distribution = \"double\" ,\n",
        "            filter_size = (3,3),\n",
        "            number_of_filters_base  = 32,\n",
        "            initializer = 'he_uniform',\n",
        "            dropout_fraction = None,\n",
        "            pool_size = (2,2),\n",
        "            padding = 'same',\n",
        "            dense_neurons = 128,\n",
        "            num_classes = 10,\n",
        "            optimizer = 'adam',\n",
        "            epochs = 5,\n",
        "            batch_size = 32, \n",
        "            img_size = IMG_SIZE\n",
        "        ) \n",
        "    wandb.init(project = 'CS6910-Assignment2-CNNs', config = config_defaults,entity='rahulsundar')\n",
        "    CONFIG = wandb.config\n",
        "        \n",
        "\n",
        "\n",
        "    wandb.run.name = \"OBJDET_\" + str(CONFIG.num_hidden_cnn_layers) + \"_dn_\" + str(CONFIG.dense_neurons) + \"_opt_\" + CONFIG.optimizer + \"_dro_\" + str(CONFIG.dropout_fraction) + \"_bs_\"+str(CONFIG.batch_size) + \"_fd_\" + CONFIG.filter_distribution\n",
        "\n",
        "    def myprint(s, path = '/content/drive/MyDrive/CS6910/Assignment2/TrainedModel/'+wandb.run.name):\n",
        "        with open(path+\"/mymodelsummary.txt\",'w+') as f:\n",
        "            print(s, file=f)\n",
        "            \n",
        "    with tf.device('/device:GPU:0'):        \n",
        "        objDetn = ObjectDetection(CONFIG.img_size, CONFIG )\n",
        "      #model = objDetn.build_cnnmodel()\n",
        "        model = objDetn.build_cnnmodelsimple()\n",
        "        model.summary()\n",
        "\n",
        "\n",
        "\n",
        "        model.compile(\n",
        "        optimizer=CONFIG.optimizer,  # Optimizer\n",
        "        # Loss function to minimize\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),#'categorical_crossentropy',\n",
        "        # List of metrics to monitor\n",
        "        metrics=['accuracy'],\n",
        "        )\n",
        "      \n",
        "        history = model.fit(\n",
        "                        train_generator,\n",
        "                        steps_per_epoch = train_generator.samples // CONFIG.batch_size,\n",
        "                        validation_data = validation_generator, \n",
        "                        validation_steps = validation_generator.samples // CONFIG.batch_size,\n",
        "                        epochs = CONFIG.epochs, \n",
        "                        callbacks=[WandbCallback()]\n",
        "                        )\n",
        "\n",
        "        model.save('./TrainedModel/'+wandb.run.name)\n",
        "        model.summary(print_fn=myprint)\n",
        "        wandb.finish()\n",
        "        return model, history\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56JrOEMmWplD",
        "outputId": "a16734d5-6e39-4945-d247-595dd86ba13f"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "281/281 [==============================] - 104s 366ms/step - loss: 2.1520 - accuracy: 0.2437 - val_loss: 2.4767 - val_accuracy: 0.1119\n",
            "Epoch 2/5\n",
            " 41/281 [===>..........................] - ETA: 1:18 - loss: 1.9613 - accuracy: 0.2786"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqO8m9MLj7c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8JCbnQjQEa-"
      },
      "source": [
        ""
      ]
    }
  ]
}