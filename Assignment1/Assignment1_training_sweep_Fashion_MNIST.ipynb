{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_training_sweep_Fashion_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNEsg8eUme9SkpCqw6fZ62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51fff7a09f19497a9d4183a9bd9fd51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_843801f7ee4b4609b60b848df017f785",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_187315b740aa430a89ee0ba9e4c4e5c0",
              "IPY_MODEL_8441cd354fe04f8fb60c6f37b49c55e1"
            ]
          }
        },
        "843801f7ee4b4609b60b848df017f785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "187315b740aa430a89ee0ba9e4c4e5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_231e5c3c48914ffcbd94692fd1dbbd70",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_992f869626a14ba58b4136c41f51733b"
          }
        },
        "8441cd354fe04f8fb60c6f37b49c55e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8f9fe45b857e42fda76253516521abfb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_970f3a8953284cdcbf415bea241bf5ed"
          }
        },
        "231e5c3c48914ffcbd94692fd1dbbd70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "992f869626a14ba58b4136c41f51733b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f9fe45b857e42fda76253516521abfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "970f3a8953284cdcbf415bea241bf5ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahulSundar/CS6910-DeepLearningFundamentals/blob/main/Assignment1/Assignment1_training_sweep_Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AABknAuK_8UD"
      },
      "source": [
        "!pip install wandb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vuMLV7jAhpo"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + np.exp(-(z)))\n",
        "\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "\n",
        "def sin(z):\n",
        "    return np.sin(z)\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(z, 0)\n",
        "\n",
        "\n",
        "def softmax(Z):\n",
        "    return np.exp(Z) / np.sum(np.exp(Z))\n",
        "\n",
        "\n",
        "def der_sigmoid(z):\n",
        "    #return sigmoid(z)*(1 - sigmoid(z))\n",
        "    return  (1.0 / (1 + np.exp(-(z))))*(1 -  1.0 / (1 + np.exp(-(z))))\n",
        "\n",
        "def der_tanh(z):\n",
        "    return 1 - np.tanh(z) ** 2\n",
        "\n",
        "\n",
        "def der_relu(z):\n",
        "    return np.heaviside(z,1) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rcbw2UiAs_M"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_hidden_layers, \n",
        "        num_hidden_neurons, \n",
        "        X_train_raw, \n",
        "        Y_train_raw,  \n",
        "        N_train, \n",
        "        X_val_raw, \n",
        "        Y_val_raw, \n",
        "        N_val,\n",
        "        X_test_raw, \n",
        "        Y_test_raw, \n",
        "        N_test,        \n",
        "        optimizer,\n",
        "        batch_size,\n",
        "        weight_decay,\n",
        "        learning_rate,\n",
        "        max_epochs,\n",
        "        activation,\n",
        "        initializer,\n",
        "        loss\n",
        "\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        Here, we initialize the FeedForwardNeuralNetwork class with the number of hidden layers, number of hidden neurons, raw training data. \n",
        "        \"\"\"\n",
        "        \n",
        "        self.num_classes = np.max(Y_train_raw) + 1  # NUM_CLASSES\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_hidden_neurons = num_hidden_neurons\n",
        "        self.output_layer_size = self.num_classes\n",
        "        self.img_height = X_train_raw.shape[1]\n",
        "        self.img_width = X_train_raw.shape[2]\n",
        "        self.img_flattened_size = self.img_height * self.img_width\n",
        "\n",
        "        # self.layers = layers\n",
        "        self.layers = (\n",
        "            [self.img_flattened_size]\n",
        "            + num_hidden_layers * [num_hidden_neurons]\n",
        "            + [self.output_layer_size]\n",
        "        )\n",
        "\n",
        "        self.N_train = N_train\n",
        "        self.N_val = N_val\n",
        "        self.N_test = N_test\n",
        "        \n",
        "\n",
        "\n",
        "        self.X_train = np.transpose(\n",
        "            X_train_raw.reshape(\n",
        "                X_train_raw.shape[0], X_train_raw.shape[1] * X_train_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "        self.X_test = np.transpose(\n",
        "            X_test_raw.reshape(\n",
        "                X_test_raw.shape[0], X_test_raw.shape[1] * X_test_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "        self.X_val = np.transpose(\n",
        "            X_val_raw.reshape(\n",
        "                X_val_raw.shape[0], X_val_raw.shape[1] * X_val_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "\n",
        "\n",
        "        self.X_train = self.X_train / 255\n",
        "        self.X_test = self.X_test / 255\n",
        "        self.X_val = self.X_val / 255\n",
        "        \n",
        "        self.Y_train = self.oneHotEncode(Y_train_raw)  # [NUM_CLASSES X NTRAIN]\n",
        "        self.Y_val = self.oneHotEncode(Y_val_raw)\n",
        "        self.Y_test = self.oneHotEncode(Y_test_raw)\n",
        "        #self.Y_shape = self.Y_train.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # self.weights, self.biases = self.initializeNeuralNet(self.layers)\n",
        "\n",
        "\n",
        "\n",
        "        self.Activations_dict = {\"SIGMOID\": sigmoid, \"TANH\": tanh, \"RELU\": relu}\n",
        "        self.DerActivation_dict = {\n",
        "            \"SIGMOID\": der_sigmoid,\n",
        "            \"TANH\": der_tanh,\n",
        "            \"RELU\": der_relu,\n",
        "        }\n",
        "\n",
        "        self.Initializer_dict = {\n",
        "            \"XAVIER\": self.Xavier_initializer,\n",
        "            \"RANDOM\": self.random_initializer,\n",
        "            \"HE\": self.He_initializer\n",
        "        }\n",
        "\n",
        "        self.Optimizer_dict = {\n",
        "            \"SGD\": self.sgdMiniBatch,\n",
        "            \"MGD\": self.mgd,\n",
        "            \"NAG\": self.nag,\n",
        "            \"RMSPROP\": self.rmsProp,\n",
        "            \"ADAM\": self.adam,\n",
        "            \"NADAM\": self.nadam,\n",
        "        }\n",
        "        \n",
        "        self.activation = self.Activations_dict[activation]\n",
        "        self.der_activation = self.DerActivation_dict[activation]\n",
        "        self.optimizer = self.Optimizer_dict[optimizer]\n",
        "        self.initializer = self.Initializer_dict[initializer]\n",
        "        self.loss_function = loss\n",
        "        self.max_epochs = max_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.weights, self.biases = self.initializeNeuralNet(self.layers)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "    # helper functions\n",
        "    def oneHotEncode(self, Y_train_raw):\n",
        "        Ydata = np.zeros((self.num_classes, Y_train_raw.shape[0]))\n",
        "        for i in range(Y_train_raw.shape[0]):\n",
        "            value = Y_train_raw[i]\n",
        "            Ydata[int(value)][i] = 1.0\n",
        "        return Ydata\n",
        "\n",
        "    # Loss functions\n",
        "    def meanSquaredErrorLoss(self, Y_true, Y_pred):\n",
        "        MSE = np.mean((Y_true - Y_pred) ** 2)\n",
        "        return MSE\n",
        "\n",
        "    def crossEntropyLoss(self, Y_true, Y_pred):\n",
        "        CE = [-Y_true[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "        crossEntropy = np.mean(CE)\n",
        "        return crossEntropy\n",
        "\n",
        "    def L2RegularisationLoss(self, weight_decay):\n",
        "        ALPHA = weight_decay\n",
        "        return ALPHA * np.sum(\n",
        "            [\n",
        "                np.linalg.norm(self.weights[str(i + 1)]) ** 2\n",
        "                for i in range(len(self.weights))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def accuracy(self, Y_true, Y_pred, data_size):\n",
        "        Y_true_label = []\n",
        "        Y_pred_label = []\n",
        "        ctr = 0\n",
        "        for i in range(data_size):\n",
        "            Y_true_label.append(np.argmax(Y_true[:, i]))\n",
        "            Y_pred_label.append(np.argmax(Y_pred[:, i]))\n",
        "            if Y_true_label[i] == Y_pred_label[i]:\n",
        "                ctr += 1\n",
        "        accuracy = ctr / data_size\n",
        "        return accuracy, Y_true_label, Y_pred_label\n",
        "\n",
        "    def Xavier_initializer(self, size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "        return np.random.normal(0, xavier_stddev, size=(out_dim, in_dim))\n",
        "\n",
        "    def random_initializer(self, size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        return np.random.normal(0, 1, size=(out_dim, in_dim))\n",
        "\n",
        "\n",
        "    def He_initializer(self,size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        He_stddev = np.sqrt(2 / (in_dim))\n",
        "        return np.random.normal(0, 1, size=(out_dim, in_dim)) * He_stddev\n",
        "\n",
        "\n",
        "    def initializeNeuralNet(self, layers):\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.initializer(size=[layers[l + 1], layers[l]])\n",
        "            b = np.zeros((layers[l + 1], 1))\n",
        "            weights[str(l + 1)] = W\n",
        "            biases[str(l + 1)] = b\n",
        "        return weights, biases\n",
        "\n",
        "    def forwardPropagate(self, X_train_batch, weights, biases):\n",
        "        \"\"\"\n",
        "        Returns the neural network given input data, weights, biases.\n",
        "        Arguments:\n",
        "                 : X - input matrix\n",
        "                 : Weights  - Weights matrix\n",
        "                 : biases - Bias vectors \n",
        "        \"\"\"\n",
        "        # Number of layers = length of weight matrix + 1\n",
        "        num_layers = len(weights) + 1\n",
        "        # A - Preactivations\n",
        "        # H - Activations\n",
        "        X = X_train_batch\n",
        "        H = {}\n",
        "        A = {}\n",
        "        H[\"0\"] = X\n",
        "        A[\"0\"] = X\n",
        "        for l in range(0, num_layers - 2):\n",
        "            if l == 0:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, X), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "            else:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, H[str(l)]), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "\n",
        "        # Here the last layer is not activated as it is a regression problem\n",
        "        W = weights[str(num_layers - 1)]\n",
        "        b = biases[str(num_layers - 1)]\n",
        "        A[str(num_layers - 1)] = np.add(np.matmul(W, H[str(num_layers - 2)]), b)\n",
        "        # Y = softmax(A[-1])\n",
        "        Y = softmax(A[str(num_layers - 1)])\n",
        "        H[str(num_layers - 1)] = Y\n",
        "        return Y, H, A\n",
        "\n",
        "    def backPropagate(\n",
        "        self, Y, H, A, Y_train_batch, weight_decay=0\n",
        "    ):\n",
        "\n",
        "        ALPHA = weight_decay\n",
        "        gradients_weights = []\n",
        "        gradients_biases = []\n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        # Gradient with respect to the output layer is absolutely fine.\n",
        "        if self.loss_function == \"CROSS\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = -(Y_train_batch - Y)\n",
        "        elif self.loss_function == \"MSE\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = np.multiply(\n",
        "                2 * (Y - Y_train_batch), np.multiply(Y, (1 - Y))\n",
        "            )\n",
        "\n",
        "        for l in range(num_layers - 2, -1, -1):\n",
        "\n",
        "            if ALPHA != 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = (\n",
        "                    np.outer(globals()[\"grad_a\" + str(l + 1)], H[str(l)])\n",
        "                    + ALPHA * self.weights[str(l + 1)]\n",
        "                )\n",
        "            elif ALPHA == 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = np.outer(\n",
        "                    globals()[\"grad_a\" + str(l + 1)], H[str(l)]\n",
        "                )\n",
        "            globals()[\"grad_b\" + str(l + 1)] = globals()[\"grad_a\" + str(l + 1)]\n",
        "            gradients_weights.append(globals()[\"grad_W\" + str(l + 1)])\n",
        "            gradients_biases.append(globals()[\"grad_b\" + str(l + 1)])\n",
        "            if l != 0:\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], self.der_activation(A[str(l)])\n",
        "                )\n",
        "            elif l == 0:\n",
        "\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], (A[str(l)])\n",
        "                )\n",
        "        return gradients_weights, gradients_biases\n",
        "\n",
        "\n",
        "    def predict(self,X,length_dataset):\n",
        "        Y_pred = []        \n",
        "        for i in range(length_dataset):\n",
        "\n",
        "            Y, H, A = self.forwardPropagate(\n",
        "                X[:, i].reshape(self.img_flattened_size, 1),\n",
        "                self.weights,\n",
        "                self.biases,\n",
        "            )\n",
        "\n",
        "            Y_pred.append(Y.reshape(self.num_classes,))\n",
        "        Y_pred = np.array(Y_pred).transpose()\n",
        "        return Y_pred\n",
        "\n",
        "    def sgd(self, epochs, length_dataset, learning_rate, weight_decay=0):\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            # perm = np.random.permutation(N)\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            deltaw = [\n",
        "                np.zeros((self.layers[l + 1], self.layers[l]))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "            deltab = [\n",
        "                np.zeros((self.layers[l + 1], 1))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "\n",
        "                Y, H, A = self.forwardPropagate(\n",
        "                    X_train[:, i].reshape(self.img_flattened_size, 1),\n",
        "                    self.weights,\n",
        "                    self.biases,\n",
        "                )\n",
        "                grad_weights, grad_biases = self.backPropagate(\n",
        "                    Y, H, A, Y_train[:, i].reshape(self.num_classes, 1)\n",
        "                )\n",
        "                deltaw = [\n",
        "                    grad_weights[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "                deltab = [\n",
        "                    grad_biases[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "\n",
        "                CE.append(\n",
        "                    self.crossEntropyLoss(\n",
        "                        self.Y_train[:, i].reshape(self.num_classes, 1), Y\n",
        "                    )\n",
        "                    + self.L2RegularisationLoss(weight_decay)\n",
        "                )\n",
        "\n",
        "                # print(num_points_seen)\n",
        "                self.weights = {\n",
        "                    str(i + 1): (self.weights[str(i + 1)] - learning_rate * deltaw[i])\n",
        "                    for i in range(len(self.weights))\n",
        "                }\n",
        "                self.biases = {\n",
        "                    str(i + 1): (self.biases[str(i + 1)] - learning_rate * deltab[i])\n",
        "                    for i in range(len(self.biases))\n",
        "                }\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "            \n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch, })\n",
        "        # data = [[epoch, loss[epoch]] for epoch in range(epochs)]\n",
        "        # table = wandb.Table(data=data, columns = [\"Epoch\", \"Loss\"])\n",
        "        # wandb.log({'loss':wandb.plot.line(table, \"Epoch\", \"Loss\", title=\"Loss vs Epoch Line Plot\")})\n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "      \n",
        "    def sgdMiniBatch(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        num_points_seen = 0\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "            \n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                    \n",
        "                    \n",
        "                    self.weights = {str(i+1):(self.weights[str(i+1)] - learning_rate*deltaw[i]/batch_size) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):(self.biases[str(i+1)] - learning_rate*deltab[i]) for i in range(len(self.biases))}\n",
        "                    \n",
        "                    #resetting gradient updates\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "\n",
        "    def mgd(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        num_points_seen = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "\n",
        "                    v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1) : (self.weights[str(i+1)] - v_w[i]) for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1): (self.biases[str(i+1)] - v_b[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    prev_v_w = v_w\n",
        "                    prev_v_b = v_b\n",
        "\n",
        "                    #resetting gradient updates\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "\n",
        "\n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        " \n",
        " \n",
        "    def stochasticNag(self,epochs,length_dataset, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []  \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            v_w = [GAMMA*prev_v_w[i] for i in range(0, len(self.layers)-1)]  \n",
        "            v_b = [GAMMA*prev_v_b[i] for i in range(0, len(self.layers)-1)]\n",
        "                        \n",
        "            for i in range(length_dataset):\n",
        "                winter = {str(i+1) : self.weights[str(i+1)] - v_w[i] for i in range(0, len(self.layers)-1)}\n",
        "                binter = {str(i+1) : self.biases[str(i+1)] - v_b[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), winter, binter) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                            \n",
        "                v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i] for i in range(num_layers - 1)]\n",
        "                v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i] for i in range(num_layers - 1)]\n",
        "        \n",
        "                self.weights = {str(i+1):self.weights[str(i+1)] - v_w[i] for i in range(len(self.weights))} \n",
        "                self.biases = {str(i+1):self.biases[str(i+1)] - v_b[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                prev_v_w = v_w\n",
        "                prev_v_b = v_b\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "    \n",
        "\n",
        "    def nag(self,epochs,length_dataset, batch_size,learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        num_points_seen = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []  \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            v_w = [GAMMA*prev_v_w[i] for i in range(0, len(self.layers)-1)]  \n",
        "            v_b = [GAMMA*prev_v_b[i] for i in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                winter = {str(i+1) : self.weights[str(i+1)] - v_w[i] for i in range(0, len(self.layers)-1)}\n",
        "                binter = {str(i+1) : self.biases[str(i+1)] - v_b[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), winter, binter) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "\n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:                            \n",
        "\n",
        "                    v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i]/batch_size for i in range(num_layers - 1)]\n",
        "        \n",
        "                    self.weights ={str(i+1):self.weights[str(i+1)]  - v_w[i] for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - v_b[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                    prev_v_w = v_w\n",
        "                    prev_v_b = v_b\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "    \n",
        "\n",
        "    \n",
        "    def rmsProp(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA = 1e-8, 0.9\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        num_points_seen = 0        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "                        \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "            \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "            \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))            \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                \n",
        "                    v_w = [BETA*v_w[i] + (1-BETA)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA*v_b[i] + (1-BETA)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "\n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)]  - deltaw[i]*(learning_rate/np.sqrt(v_w[i]+EPS)) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - deltab[i]*(learning_rate/np.sqrt(v_b[i]+EPS)) for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred  \n",
        "\n",
        "\n",
        "\n",
        "    def adam(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        \n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA1, BETA2 = 1e-8, 0.9, 0.99\n",
        "        \n",
        "        m_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "        \n",
        "        m_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]   \n",
        "        \n",
        "        num_points_seen = 0 \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "           \n",
        "            for i in range(length_dataset):\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))                 \n",
        "\n",
        "                num_points_seen += 1\n",
        "                ctr = 0\n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                    ctr += 1\n",
        "                \n",
        "                    m_w = [BETA1*m_w[i] + (1-BETA1)*deltaw[i] for i in range(num_layers - 1)]\n",
        "                    m_b = [BETA1*m_b[i] + (1-BETA1)*deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                    v_w = [BETA2*v_w[i] + (1-BETA2)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA2*v_b[i] + (1-BETA2)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    m_w_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    m_b_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                \n",
        "                    v_w_hat = [v_w[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    v_b_hat = [v_b[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/np.sqrt(v_w[i]+EPS))*m_w_hat[i] for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/np.sqrt(v_b[i]+EPS))*m_b_hat[i] for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "    \n",
        "    def nadam(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        GAMMA, EPS, BETA1, BETA2 = 0.9, 1e-8, 0.9, 0.99\n",
        "\n",
        "        m_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "\n",
        "        m_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)] \n",
        "\n",
        "        num_points_seen = 0 \n",
        "        \n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "\n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "\n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))   \n",
        "                num_points_seen += 1\n",
        "                \n",
        "                if num_points_seen % batch_size == 0:\n",
        "                    \n",
        "                    m_w = [BETA1*m_w[i] + (1-BETA1)*deltaw[i] for i in range(num_layers - 1)]\n",
        "                    m_b = [BETA1*m_b[i] + (1-BETA1)*deltab[i] for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    v_w = [BETA2*v_w[i] + (1-BETA2)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA2*v_b[i] + (1-BETA2)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    m_w_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    m_b_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                    \n",
        "                    v_w_hat = [v_w[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    v_b_hat = [v_b[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/(np.sqrt(v_w_hat[i])+EPS))*(BETA1*m_w_hat[i]+ (1-BETA1)*deltaw[i]) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/(np.sqrt(v_b_hat[i])+EPS))*(BETA1*m_b_hat[i] + (1-BETA1)*deltab[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "             \n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred  \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_3tVRXJA8Rv",
        "outputId": "7aa5fd4b-0aab-4e82-b9ed-591dc64f00e0"
      },
      "source": [
        "import wandb\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "#from feedForwardNeuralNet import FeedForwardNeuralNetwork\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "\n",
        "    # Load the data in predefined train and test split ratios:\n",
        "#    __spec__ = \"ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>)\"\n",
        "(trainIn, trainOut), (testIn, testOut) = fashion_mnist.load_data()\n",
        "\n",
        "N_train_full = trainOut.shape[0]\n",
        "N_train = int(0.9*N_train_full)\n",
        "N_validation = int(0.1 * trainOut.shape[0])\n",
        "N_test = testOut.shape[0]\n",
        "\n",
        "\n",
        "idx  = np.random.choice(trainOut.shape[0], N_train_full, replace=False)\n",
        "idx2 = np.random.choice(testOut.shape[0], N_test, replace=False)\n",
        "\n",
        "trainInFull = trainIn[idx, :]\n",
        "trainOutFull = trainOut[idx]\n",
        "\n",
        "trainIn = trainInFull[:N_train,:]\n",
        "trainOut = trainOutFull[:N_train]\n",
        "\n",
        "validIn = trainInFull[N_train:, :]\n",
        "validOut = trainOutFull[N_train:]    \n",
        "\n",
        "testIn = testIn[idx2, :]\n",
        "testOut = testOut[idx2]\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"validationaccuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"max_epochs\": {\n",
        "            \"values\": [5, 10]\n",
        "        },\n",
        "\n",
        "        \"initializer\": {\n",
        "            \"values\": [\"RANDOM\", \"XAVIER\"]\n",
        "        },\n",
        "\n",
        "        \"num_layers\": {\n",
        "            \"values\": [2, 3, 4]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"num_hidden_neurons\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \n",
        "        \"activation\": {\n",
        "            \"values\": ['RELU', 'SIGMOID', 'TANH']\n",
        "        },\n",
        "        \n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001, 0.0001]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0, 0.0005,0.5]\n",
        "        },\n",
        "        \n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"SGD\", \"MGD\", \"NAG\", \"RMSPROP\", \"ADAM\",\"NADAM\"]\n",
        "        },\n",
        "                    \n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "        }\n",
        "        \n",
        "        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='CS6910-DeepLearningFundamentals-Assignment1', entity='rahulsundar')\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: ck9g3rau\n",
            "Sweep URL: https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/ck9g3rau\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyC4fPR8Cwgp"
      },
      "source": [
        "def train():    \n",
        "    config_defaults = dict(\n",
        "            max_epochs=5,\n",
        "            num_hidden_layers=3,\n",
        "            num_hidden_neurons=32,\n",
        "            weight_decay=0,\n",
        "            learning_rate=1e-3,\n",
        "            optimizer=\"MGD\",\n",
        "            batch_size=16,\n",
        "            activation=\"TANH\",\n",
        "            initializer=\"RANDOM\",\n",
        "            loss=\"CROSS\",\n",
        "        )\n",
        "        \n",
        "    #wandb.init(project='CS6910-DeepLearningFundamentals-Assignment1', entity='rahulsundar', config = config_defaults)\n",
        "    wandb.init(config = config_defaults)\n",
        "\n",
        "\n",
        "    wandb.run.name = \"hl_\" + str(wandb.config.num_hidden_layers) + \"_hn_\" + str(wandb.config.num_hidden_neurons) + \"_opt_\" + wandb.config.optimizer + \"_act_\" + wandb.config.activation + \"_lr_\" + str(wandb.config.learning_rate) + \"_bs_\"+str(wandb.config.batch_size) + \"_init_\" + wandb.config.initializer + \"_ep_\"+ str(wandb.config.max_epochs)+ \"_l2_\" + str(wandb.config.weight_decay) \n",
        "    CONFIG = wandb.config\n",
        "\n",
        "\n",
        "    \n",
        "    #sweep_id = wandb.sweep(sweep_config)\n",
        "  \n",
        "\n",
        "    FFNN = FeedForwardNeuralNetwork(\n",
        "        num_hidden_layers=CONFIG.num_hidden_layers,\n",
        "        num_hidden_neurons=CONFIG.num_hidden_neurons,\n",
        "        X_train_raw=trainIn,\n",
        "        Y_train_raw=trainOut,\n",
        "        N_train = N_train,\n",
        "        X_val_raw = validIn,\n",
        "        Y_val_raw = validOut,\n",
        "        N_val = N_validation,\n",
        "        X_test_raw = testIn,\n",
        "        Y_test_raw = testOut,\n",
        "        N_test = N_test,\n",
        "        optimizer = CONFIG.optimizer,\n",
        "        batch_size = CONFIG.batch_size,\n",
        "        weight_decay = CONFIG.weight_decay,\n",
        "        learning_rate = CONFIG.learning_rate,\n",
        "        max_epochs = CONFIG.max_epochs,\n",
        "        activation = CONFIG.activation,\n",
        "        initializer = CONFIG.initializer,\n",
        "        loss = CONFIG.loss\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.learning_rate)\n",
        " "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "51fff7a09f19497a9d4183a9bd9fd51d",
            "843801f7ee4b4609b60b848df017f785",
            "187315b740aa430a89ee0ba9e4c4e5c0",
            "8441cd354fe04f8fb60c6f37b49c55e1",
            "231e5c3c48914ffcbd94692fd1dbbd70",
            "992f869626a14ba58b4136c41f51733b",
            "8f9fe45b857e42fda76253516521abfb",
            "970f3a8953284cdcbf415bea241bf5ed"
          ]
        },
        "id": "z4EENrgtDTnu",
        "outputId": "1ce6a771-2785-47fc-9762-c46a37ecd3bc"
      },
      "source": [
        "wandb.agent(sweep_id, train, count = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6nfqy7g6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: TANH\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: RANDOM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: NAG\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulsundar\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">dauntless-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/ck9g3rau\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/ck9g3rau</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/6nfqy7g6\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/6nfqy7g6</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210314_092517-6nfqy7g6</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 2.635e-01, Training accuracy:0.51, Validation Accuracy: 0.52, Time: 47.55, Learning Rate: 1.000e-03\n",
            "Epoch: 1, Loss: 1.373e-01, Training accuracy:0.57, Validation Accuracy: 0.56, Time: 48.01, Learning Rate: 1.000e-03\n",
            "Epoch: 2, Loss: 1.167e-01, Training accuracy:0.60, Validation Accuracy: 0.60, Time: 47.95, Learning Rate: 1.000e-03\n",
            "Epoch: 3, Loss: 1.068e-01, Training accuracy:0.62, Validation Accuracy: 0.62, Time: 48.19, Learning Rate: 1.000e-03\n",
            "Epoch: 4, Loss: 1.010e-01, Training accuracy:0.63, Validation Accuracy: 0.62, Time: 48.20, Learning Rate: 1.000e-03\n",
            "Epoch: 5, Loss: 9.692e-02, Training accuracy:0.64, Validation Accuracy: 0.64, Time: 48.22, Learning Rate: 1.000e-03\n",
            "Epoch: 6, Loss: 9.374e-02, Training accuracy:0.65, Validation Accuracy: 0.64, Time: 48.13, Learning Rate: 1.000e-03\n",
            "Epoch: 7, Loss: 9.099e-02, Training accuracy:0.65, Validation Accuracy: 0.65, Time: 47.76, Learning Rate: 1.000e-03\n",
            "Epoch: 8, Loss: 8.873e-02, Training accuracy:0.66, Validation Accuracy: 0.66, Time: 47.29, Learning Rate: 1.000e-03\n",
            "Epoch: 9, Loss: 8.678e-02, Training accuracy:0.66, Validation Accuracy: 0.66, Time: 47.05, Learning Rate: 1.000e-03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 222<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51fff7a09f19497a9d4183a9bd9fd51d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210314_092517-6nfqy7g6/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210314_092517-6nfqy7g6/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>loss</td><td>0.08678</td></tr><tr><td>trainingaccuracy</td><td>0.66452</td></tr><tr><td>validationaccuracy</td><td>0.66267</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>_runtime</td><td>541</td></tr><tr><td>_timestamp</td><td>1615714458</td></tr><tr><td>_step</td><td>9</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>trainingaccuracy</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>validationaccuracy</td><td>▁▃▅▆▆▇▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>_runtime</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>_timestamp</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>_step</td><td>▁▂▃▃▄▅▆▆▇█</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">dauntless-sweep-1</strong>: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/6nfqy7g6\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/6nfqy7g6</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ugnqfb6o with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: TANH\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: RANDOM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: NAG\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">restful-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/ck9g3rau\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/ck9g3rau</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/ugnqfb6o\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/ugnqfb6o</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210314_093434-ugnqfb6o</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}