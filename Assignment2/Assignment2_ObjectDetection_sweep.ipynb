{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_ObjectDetection_unbalanced_sweep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPc3M/1a0r+781gKgvxofh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahulSundar/CS6910-DeepLearningFundamentals/blob/main/Assignment2/Assignment2_ObjectDetection_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zGZzbEmQ48A"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTTrz8WoTNlV"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JMG1uDaRGZr"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrHj1bbxHjVi"
      },
      "source": [
        "!pip install wandb\n",
        "\n",
        "!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
        "!unzip nature_12K.zip\n",
        "!rm nature_12K.zip\n",
        "\n",
        "!mv ./inaturalist_12K/val ./inaturalist_12K/test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cykm-daIJXW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oqxnJgOIJO8"
      },
      "source": [
        "import os, sys\n",
        "sys.path.append(\n",
        "\"/content/drive/MyDrive/CS6910/Assignment2\"\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cGzCjsKIJAl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52aBib4BNSiC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPECnwAsHjne"
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#import keras\n",
        "\n",
        "# keras pre-trained models\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2 as IRV2\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input, InputLayer, Flatten, Conv2D, BatchNormalization, MaxPooling2D, Activation , GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential,  Model\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "class ObjectDetection():\n",
        "\n",
        "    def __init__(self, IMG_SIZE, modelConfigDict, using_pretrained_model = False, base_model = \"IRV2\" ):\n",
        "        \n",
        "        self.num_hidden_cnn_layers= modelConfigDict[\"num_hidden_cnn_layers\"]\n",
        "        self.activation = modelConfigDict[\"activation\"]\n",
        "        self.batch_normalization = modelConfigDict[\"batch_normalization\"]\n",
        "        self.filter_distribution = modelConfigDict[\"filter_distribution\"]\n",
        "        self.filter_size = modelConfigDict[\"filter_size\"]\n",
        "        self.number_of_filters_base  = modelConfigDict[\"number_of_filters_base\"]\n",
        "        self.dropout_fraction = modelConfigDict[\"dropout_fraction\"]\n",
        "        self.pool_size = modelConfigDict[\"pool_size\"]\n",
        "        self.padding = modelConfigDict[\"padding\"]\n",
        "        self.dense_neurons = modelConfigDict[\"dense_neurons\"]\n",
        "        self.num_classes = modelConfigDict[\"num_classes\"]\n",
        "        self.optimizer = modelConfigDict[\"optimizer\"]\n",
        "        self.global_average_pooling = modelConfigDict[\"global_average_pooling\"]\n",
        "        self.batch_normalisation_location = modelConfigDict[\"batch_normalisation_location\"]\n",
        "        BASE_MODELS = {\n",
        "                          \"IRV2\" : IRV2,\n",
        "                          \"IV3\" : InceptionV3,\n",
        "                          \"RN50\" : ResNet50,\n",
        "                          \"XCPTN\" : Xception\n",
        "                      }      \n",
        "        \n",
        "        if using_pretrained_model == True:\n",
        "            self.base_model = base_model\n",
        "            if self.base_model == \"RN50\":\n",
        "                self.IMG_HEIGHT = 224\n",
        "                self.IMG_WIDTH = 224\n",
        "            else:\n",
        "                self.IMG_HEIGHT = IMG_SIZE[0]\n",
        "                self.IMG_WIDTH = IMG_SIZE[1]        \n",
        "\n",
        "        self.IMG_HEIGHT = IMG_SIZE[0]\n",
        "        self.IMG_WIDTH = IMG_SIZE[1]        \n",
        "         \n",
        "        self.input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)\n",
        "\n",
        "\n",
        "    def build_cnnmodel_conv(self):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            \n",
        "            #First CNN layer connecting to input layer\n",
        "            model.add(Conv2D(self.number_of_filters_base, self.filter_size, padding = self.padding,kernel_initializer = \"he_uniform\", input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)))\n",
        "            if self.batch_normalisation_location == \"Before\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "            model.add(Activation(self.activation))\n",
        "            \n",
        "            #batch_normalisation\n",
        "            if self.batch_normalisation_location == \"After\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "            #max pooling\n",
        "            model.add(MaxPooling2D(pool_size=self.pool_size))  \n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            for i in range(self.num_hidden_cnn_layers-1):\n",
        "                #i+2th Convolutional Layer\n",
        "            \n",
        "                ## Standard filter distribution - same number of filters in all Convolutional layers\n",
        "                if self.filter_distribution == \"standard\":\n",
        "                    model.add(Conv2D(self.number_of_filters_base, self.filter_size,kernel_initializer = \"he_uniform\",padding = self.padding))\n",
        "            \n",
        "                ## Double filter distribution - double number of filters in each Convolutional layers\n",
        "                elif self.filter_distribution == \"double\":\n",
        "                    model.add(Conv2D(2**(i+1)*self.number_of_filters_base, self.filter_size,kernel_initializer = \"he_uniform\", padding = self.padding))\n",
        "            \n",
        "                ## Halve the filter size in each successive convolutional layers\n",
        "                elif self.filter_distribution == \"half\":\n",
        "                    model.add(Conv2D(int(self.number_of_filters_base/2**(i+1)), self.filter_size,kernel_initializer = \"he_uniform\", padding = self.padding))\n",
        "            \n",
        "                if self.batch_normalisation_location == \"Before\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "                model.add(Activation(self.activation))\n",
        "            \n",
        "                if self.batch_normalisation_location == \"After\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "            \n",
        "                model.add(MaxPooling2D(pool_size=self.pool_size))\n",
        "                if self.dropout_fraction != None:\n",
        "                    model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            \n",
        "            #Final densely connected layers\n",
        "            if self.global_average_pooling == True:\n",
        "                model.add(GlobalAveragePooling2D())\n",
        "            else:\n",
        "                model.add(Flatten())\n",
        "\n",
        "            model.add(Dense(self.dense_neurons, activation = 'sigmoid'))\n",
        "            model.add(Dense(self.num_classes, activation = 'softmax'))\n",
        "\n",
        "            return model      \n",
        "        \n",
        "    def build_cnnmodel_all(self):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            \n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            \n",
        "            #First CNN layer connecting to input layer\n",
        "            model.add(Conv2D(self.number_of_filters_base, self.filter_size, padding = self.padding,kernel_initializer = \"he_uniform\", input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)))\n",
        "            model.add(Activation(self.activation))\n",
        "            \n",
        "            #batch_normalisation\n",
        "            if self.batch_normalisation_location == \"After\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "            #max pooling\n",
        "            model.add(MaxPooling2D(pool_size=self.pool_size))  \n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            for i in range(self.num_hidden_cnn_layers-1):\n",
        "                #i+2th Convolutional Layer\n",
        "            \n",
        "                ## Standard filter distribution - same number of filters in all Convolutional layers\n",
        "                if self.filter_distribution == \"standard\":\n",
        "                    model.add(Conv2D(self.number_of_filters_base, self.filter_size,kernel_initializer = \"he_uniform\", padding = self.padding))\n",
        "            \n",
        "                ## Double filter distribution - double number of filters in each Convolutional layers\n",
        "                elif self.filter_distribution == \"double\":\n",
        "                    model.add(Conv2D(2**(i+1)*self.number_of_filters_base, self.filter_size,kernel_initializer = \"he_uniform\",padding = self.padding))\n",
        "            \n",
        "                ## Halve the filter size in each successive convolutional layers\n",
        "                elif self.filter_distribution == \"half\":\n",
        "                    model.add(Conv2D(int(self.number_of_filters_base/2**(i+1)), self.filter_size,kernel_initializer = \"he_uniform\", padding = self.padding))\n",
        "            \n",
        "                model.add(Activation(self.activation))\n",
        "            \n",
        "                if self.batch_normalisation_location == \"After\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "            \n",
        "                model.add(MaxPooling2D(pool_size=self.pool_size))\n",
        "                if self.dropout_fraction != None:\n",
        "                    model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            \n",
        "            #Final densely connected layers\n",
        "            if self.global_average_pooling == True:\n",
        "                model.add(GlobalAveragePooling2D())\n",
        "            else:\n",
        "                model.add(Flatten())\n",
        "\n",
        "            model.add(Dense(self.dense_neurons, activation = 'sigmoid'))\n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            model.add(Dense(self.num_classes, activation = 'softmax'))\n",
        "            \n",
        "            return model      \n",
        "      \n",
        "    def build_cnnmodel_dense(self):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            \n",
        "            #First CNN layer connecting to input layer\n",
        "            model.add(Conv2D(self.number_of_filters_base, self.filter_size ,kernel_initializer = \"he_uniform\",padding = self.padding,input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)))\n",
        "            if self.batch_normalisation_location == \"Before\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "            model.add(Activation(self.activation))\n",
        "            \n",
        "            #batch_normalisation\n",
        "            if self.batch_normalisation_location == \"After\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "            #max pooling\n",
        "            model.add(MaxPooling2D(pool_size=self.pool_size))  \n",
        "            for i in range(self.num_hidden_cnn_layers-1):\n",
        "                #i+2th Convolutional Layer\n",
        "            \n",
        "                ## Standard filter distribution - same number of filters in all Convolutional layers\n",
        "                if self.filter_distribution == \"standard\":\n",
        "                    model.add(Conv2D(self.number_of_filters_base, self.filter_size,kernel_initializer = \"he_uniform\",padding = self.padding))\n",
        "            \n",
        "                ## Double filter distribution - double number of filters in each Convolutional layers\n",
        "                elif self.filter_distribution == \"double\":\n",
        "                    model.add(Conv2D(2**(i+1)*self.number_of_filters_base, self.filter_size,kernel_initializer = \"he_uniform\",padding = self.padding))\n",
        "            \n",
        "                ## Halve the filter size in each successive convolutional layers\n",
        "                elif self.filter_distribution == \"half\":\n",
        "                    model.add(Conv2D(int(self.number_of_filters_base/2**(i+1)),self.filter_size, kernel_initializer = \"he_uniform\"))\n",
        "            \n",
        "                if self.batch_normalisation_location == \"Before\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "                model.add(Activation(self.activation))\n",
        "            \n",
        "                if self.batch_normalisation_location == \"After\" and self.batch_normalization: model.add(BatchNormalization())\n",
        "                \n",
        "                model.add(MaxPooling2D(pool_size=self.pool_size))\n",
        "            \n",
        "            #Final densely connected layers\n",
        "            if self.global_average_pooling == True:\n",
        "                model.add(GlobalAveragePooling2D())\n",
        "            else:\n",
        "                model.add(Flatten())\n",
        "            model.add(Dense(self.dense_neurons, activation = 'sigmoid'))\n",
        "            if self.dropout_fraction != None:\n",
        "                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n",
        "            model.add(Dense(self.num_classes, activation = 'softmax'))\n",
        "          \n",
        "            return model \n",
        "                 \n",
        "        \n",
        "    def load_pretrained_model(self):\n",
        "        base_model = BASE_MODELS[self.base_model_name]\n",
        "        base = base_model(weights='imagenet', include_top=False)\n",
        "        x = base.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(self.dense_neurons, activation='relu')(x)\n",
        "        guesses = Dense(self.num_classes, activation='softmax')(x)\n",
        "        model = Model(inputs=base.input, outputs=guesses)\n",
        "\n",
        "        # freeze all base layers\n",
        "        for layer in base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        #model.compile(optimizer=self.optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og5Ds0aQIHWI"
      },
      "source": [
        "# data preprocessing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import pathlib\n",
        "\n",
        "#wandb logging\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "\n",
        "#physical_devices = tf.config.list_physical_devices('GPU')\n",
        "#try:\n",
        "#  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "#except:\n",
        " # Invalid device or cannot modify virtual devices once initialized.\n",
        "#  pass\n",
        "\n",
        "\n",
        "IMG_SIZE = (128,128)\n",
        "\n",
        "'''\n",
        "#sweep config\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"val_accuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  'early_terminate': {\n",
        "        'type':'hyperband',\n",
        "        'min_iter': [3],\n",
        "        's': [2]\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \n",
        "        \"activation\":{\n",
        "            \"values\": [\"relu\", \"elu\", \"selu\"]\n",
        "        },\n",
        "        \"filter_size\": {\n",
        "            \"values\": [(2,2), (3,3), (4,4)]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32, 64]\n",
        "        },\n",
        "        \"padding\": {\n",
        "            \"values\": [\"same\",\"valid\"]\n",
        "        },\n",
        "        \"data_augmentation\": {\n",
        "            \"values\": [True, False]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\", \"adam\", \"rmsprop\", \"nadam\"]\n",
        "        },\n",
        "        \"batch_normalization\": {\n",
        "            \"values\": [True, False]\n",
        "        },\n",
        "        \"batch_normalisation_location\": {\n",
        "            \"values\": [\"Before\", \"After\"]\n",
        "        },\n",
        "        \"number_of_filters_base\": {\n",
        "            \"values\": [32, 64]\n",
        "        },\n",
        "        \"dense_neurons\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },   \n",
        "        \"dropout_location\": {\n",
        "            \"values\": [\"conv\",\"dense\",\"all\"]\n",
        "        },\n",
        "        \"dropout_fraction\": {\n",
        "            \"values\": [None, 0.2,0.3]\n",
        "        },  \n",
        "        \"global_average_pooling\": {\n",
        "            \"values\": [False,True]\n",
        "        },        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='CS6910-Assignment2-CNNs', entity='rahulsundar')\n",
        "'''\n",
        "\n",
        "\n",
        "#train function\n",
        "def train():\n",
        "\n",
        "        \n",
        "    config_defaults = dict(\n",
        "            num_hidden_cnn_layers = 5 ,\n",
        "            activation = 'relu',\n",
        "            batch_normalization = True,\n",
        "            batch_normalisation_location = \"After\",\n",
        "            filter_distribution = \"double\" ,\n",
        "            filter_size = (3,3),\n",
        "            number_of_filters_base  = 32,\n",
        "            dropout_fraction = None,\n",
        "            dropout_location = \"dense\",\n",
        "            pool_size = (2,2),\n",
        "            padding = 'same',\n",
        "            dense_neurons = 128,\n",
        "            num_classes = 10,\n",
        "            optimizer = 'adam',\n",
        "            epochs = 5,\n",
        "            batch_size = 32, \n",
        "            data_augmentation = False,\n",
        "            global_average_pooling = True,\n",
        "            img_size = IMG_SIZE\n",
        "        ) \n",
        "\n",
        "\n",
        "\n",
        "    #wandb.init( config = config_defaults)\n",
        "    wandb.init(project = 'CS6910-Assignment2-CNNs', config = config_defaults,entity='rahulsundar')\n",
        "    CONFIG = wandb.config\n",
        "\n",
        "\n",
        "    wandb.run.name = \"OBJDET_\" + str(CONFIG.num_hidden_cnn_layers) + \"_dn_\" + str(CONFIG.dense_neurons) + \"_opt_\" + CONFIG.optimizer + \"_dro_\" + str(CONFIG.dropout_fraction) + \"_bs_\"+str(CONFIG.batch_size) + \"_fd_\" + CONFIG.filter_distribution + \"_bnl_\" + CONFIG.batch_normalisation_location + \"_dpl_\" + CONFIG.dropout_location\n",
        "\n",
        "    data_augmentation = CONFIG.data_augmentation\n",
        "\n",
        "    \n",
        "    BATCH_SIZE = CONFIG.batch_size\n",
        "\n",
        "\n",
        "    if data_augmentation == True:\n",
        "\n",
        "    #Faster Alternative\n",
        "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255,\n",
        "                validation_split = 0.1,\n",
        "                shear_range=0.2,\n",
        "                zoom_range=0.2,\n",
        "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "                samplewise_center=False,  # set each sample mean to 0\n",
        "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "                samplewise_std_normalization=False,  # divide each input by its std\n",
        "                zca_whitening=False,  # apply ZCA whitening\n",
        "                rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "                horizontal_flip=True,  # randomly flip images\n",
        "                vertical_flip=False\n",
        "                )\n",
        "    else:\n",
        "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,validation_split = 0.1)\n",
        "\n",
        "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        './inaturalist_12K/train',\n",
        "        subset='training',\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        shuffle = True,\n",
        "        seed = 123)\n",
        "        \n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "            './inaturalist_12K/train',\n",
        "            target_size=IMG_SIZE,\n",
        "            subset = 'validation',\n",
        "            batch_size=BATCH_SIZE,\n",
        "            class_mode='categorical',\n",
        "            shuffle = True,\n",
        "            seed = 123)\n",
        "\n",
        "\n",
        "            \n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "            './inaturalist_12K/test',\n",
        "            target_size=IMG_SIZE,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            class_mode='categorical',\n",
        "            shuffle = True,\n",
        "            seed = 123)\n",
        "\n",
        "\n",
        "\n",
        "    with tf.device('/device:GPU:0'):        \n",
        "        objDetn = ObjectDetection(CONFIG.img_size, CONFIG )\n",
        "        if CONFIG.dropout_location == \"all\":\n",
        "            model = objDetn.build_cnnmodel_all()\n",
        "        elif CONFIG.dropout_location == \"conv\":\n",
        "            model = objDetn.build_cnnmodel_conv()\n",
        "        elif CONFIG.dropout_location == \"dense\":\n",
        "            model = objDetn.build_cnnmodel_dense()\n",
        "        \n",
        "        model.summary()\n",
        "\n",
        "\n",
        "\n",
        "        model.compile(\n",
        "        optimizer=CONFIG.optimizer,  # Optimizer\n",
        "        # Loss function to minimize\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),#'categorical_crossentropy',\n",
        "        # List of metrics to monitor\n",
        "        metrics=['accuracy'],\n",
        "        )\n",
        "      \n",
        "        history = model.fit(\n",
        "                        train_generator,\n",
        "                        steps_per_epoch = train_generator.samples // CONFIG.batch_size,\n",
        "                        validation_data = validation_generator, \n",
        "                        validation_steps = validation_generator.samples // CONFIG.batch_size,\n",
        "                        epochs = CONFIG.epochs, \n",
        "                        callbacks=[WandbCallback()]\n",
        "                        )\n",
        "\n",
        "        model.save('./TrainedModel/'+wandb.run.name)\n",
        "        wandb.finish()\n",
        "        return model, history\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56JrOEMmWplD"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqO8m9MLj7c"
      },
      "source": [
        "#wandb.agent(sweep_id, train, count = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfMqdTMFz0b1"
      },
      "source": [
        "!cp -rf /content/TrainedModel/OBJDET_5_dn_128_opt_nadam_dro_None_bs_32_fd_double_bnl_After_dpl_all  /content/drive/MyDrive/CS6910/Assignment2/Best_trained_Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8JCbnQjQEa-"
      },
      "source": [
        ""
      ]
    }
  ]
}