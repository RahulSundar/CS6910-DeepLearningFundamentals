{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_training_sweep_Fashion_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpFVbUjm5UmIboi/8JJ6GO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahulSundar/CS6910-DeepLearningFundamentals/blob/main/Assignment1/Assignment1_training_sweep_Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AABknAuK_8UD",
        "outputId": "5149d780-ee26-4042-f86b-b69ebda21332"
      },
      "source": [
        "!pip install wandb\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\r\u001b[K     |▏                               | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 20.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 11.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 5.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 9.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 7.4MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 7.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 7.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 7.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 163kB 7.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 7.4MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 7.4MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 204kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 235kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 7.4MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 276kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 296kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 307kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 327kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 348kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 358kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 368kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 378kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 389kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 399kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 409kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 419kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 430kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 440kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 460kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 471kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 481kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 491kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 501kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 522kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 532kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 542kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 552kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 563kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 583kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 593kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 604kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 614kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 624kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 634kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 645kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 655kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 665kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 675kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 686kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 696kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 706kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 716kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 727kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 737kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 747kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 757kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 768kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 778kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 788kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 798kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 808kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 819kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 829kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 839kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 849kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 860kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 870kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 880kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 890kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 901kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 911kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 921kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 931kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 942kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 952kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 962kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 972kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 983kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 993kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.2MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.4MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.7MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.9MB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0MB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 48.0MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=8e0454ee48a75bd1f8d22587dfd21c5163b988ee956c456a1e51b581830aab2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=87994646b6dc4f5dfc9d9158b76849c9b5d3a9eb89692f454e0490037aebf1c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: docker-pycreds, subprocess32, smmap, gitdb, GitPython, configparser, shortuuid, sentry-sdk, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vuMLV7jAhpo"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + np.exp(-(z)))\n",
        "\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "\n",
        "def sin(z):\n",
        "    return np.sin(z)\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(z, 0)\n",
        "\n",
        "\n",
        "def softmax(Z):\n",
        "    return np.exp(Z) / np.sum(np.exp(Z))\n",
        "\n",
        "\n",
        "def der_sigmoid(z):\n",
        "    #return sigmoid(z)*(1 - sigmoid(z))\n",
        "    return  (1.0 / (1 + np.exp(-(z))))*(1 -  1.0 / (1 + np.exp(-(z))))\n",
        "\n",
        "def der_tanh(z):\n",
        "    return 1 - np.tanh(z) ** 2\n",
        "\n",
        "\n",
        "def der_relu(z):\n",
        "    return np.heaviside(z,1) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rcbw2UiAs_M"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_hidden_layers, \n",
        "        num_hidden_neurons, \n",
        "        X_train_raw, \n",
        "        Y_train_raw,  \n",
        "        N_train, \n",
        "        X_val_raw, \n",
        "        Y_val_raw, \n",
        "        N_val,\n",
        "        X_test_raw, \n",
        "        Y_test_raw, \n",
        "        N_test,        \n",
        "        optimizer,\n",
        "        batch_size,\n",
        "        weight_decay,\n",
        "        learning_rate,\n",
        "        max_epochs,\n",
        "        activation,\n",
        "        initializer,\n",
        "        loss\n",
        "\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        Here, we initialize the FeedForwardNeuralNetwork class with the number of hidden layers, number of hidden neurons, raw training data. \n",
        "        \"\"\"\n",
        "        \n",
        "        self.num_classes = np.max(Y_train_raw) + 1  # NUM_CLASSES\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_hidden_neurons = num_hidden_neurons\n",
        "        self.output_layer_size = self.num_classes\n",
        "        self.img_height = X_train_raw.shape[1]\n",
        "        self.img_width = X_train_raw.shape[2]\n",
        "        self.img_flattened_size = self.img_height * self.img_width\n",
        "\n",
        "        # self.layers = layers\n",
        "        self.layers = (\n",
        "            [self.img_flattened_size]\n",
        "            + num_hidden_layers * [num_hidden_neurons]\n",
        "            + [self.output_layer_size]\n",
        "        )\n",
        "\n",
        "        self.N_train = N_train\n",
        "        self.N_val = N_val\n",
        "        self.N_test = N_test\n",
        "        \n",
        "\n",
        "\n",
        "        self.X_train = np.transpose(\n",
        "            X_train_raw.reshape(\n",
        "                X_train_raw.shape[0], X_train_raw.shape[1] * X_train_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "        self.X_test = np.transpose(\n",
        "            X_test_raw.reshape(\n",
        "                X_test_raw.shape[0], X_test_raw.shape[1] * X_test_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "        self.X_val = np.transpose(\n",
        "            X_val_raw.reshape(\n",
        "                X_val_raw.shape[0], X_val_raw.shape[1] * X_val_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "\n",
        "\n",
        "        self.X_train = self.X_train / 255\n",
        "        self.X_test = self.X_test / 255\n",
        "        self.X_val = self.X_val / 255\n",
        "        \n",
        "        self.Y_train = self.oneHotEncode(Y_train_raw)  # [NUM_CLASSES X NTRAIN]\n",
        "        self.Y_val = self.oneHotEncode(Y_val_raw)\n",
        "        self.Y_test = self.oneHotEncode(Y_test_raw)\n",
        "        #self.Y_shape = self.Y_train.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # self.weights, self.biases = self.initializeNeuralNet(self.layers)\n",
        "\n",
        "\n",
        "\n",
        "        self.Activations_dict = {\"SIGMOID\": sigmoid, \"TANH\": tanh, \"RELU\": relu}\n",
        "        self.DerActivation_dict = {\n",
        "            \"SIGMOID\": der_sigmoid,\n",
        "            \"TANH\": der_tanh,\n",
        "            \"RELU\": der_relu,\n",
        "        }\n",
        "\n",
        "        self.Initializer_dict = {\n",
        "            \"XAVIER\": self.Xavier_initializer,\n",
        "            \"RANDOM\": self.random_initializer,\n",
        "            \"HE\": self.He_initializer\n",
        "        }\n",
        "\n",
        "        self.Optimizer_dict = {\n",
        "            \"SGD\": self.sgdMiniBatch,\n",
        "            \"MGD\": self.mgd,\n",
        "            \"NAG\": self.nag,\n",
        "            \"RMSPROP\": self.rmsProp,\n",
        "            \"ADAM\": self.adam,\n",
        "            \"NADAM\": self.nadam,\n",
        "        }\n",
        "        \n",
        "        self.activation = self.Activations_dict[activation]\n",
        "        self.der_activation = self.DerActivation_dict[activation]\n",
        "        self.optimizer = self.Optimizer_dict[optimizer]\n",
        "        self.initializer = self.Initializer_dict[initializer]\n",
        "        self.loss_function = loss\n",
        "        self.max_epochs = max_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.weights, self.biases = self.initializeNeuralNet(self.layers)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "    # helper functions\n",
        "    def oneHotEncode(self, Y_train_raw):\n",
        "        Ydata = np.zeros((self.num_classes, Y_train_raw.shape[0]))\n",
        "        for i in range(Y_train_raw.shape[0]):\n",
        "            value = Y_train_raw[i]\n",
        "            Ydata[int(value)][i] = 1.0\n",
        "        return Ydata\n",
        "\n",
        "    # Loss functions\n",
        "    def meanSquaredErrorLoss(self, Y_true, Y_pred):\n",
        "        MSE = np.mean((Y_true - Y_pred) ** 2)\n",
        "        return MSE\n",
        "\n",
        "    def crossEntropyLoss(self, Y_true, Y_pred):\n",
        "        CE = [-Y_true[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "        crossEntropy = np.mean(CE)\n",
        "        return crossEntropy\n",
        "\n",
        "    def L2RegularisationLoss(self, weight_decay):\n",
        "        ALPHA = weight_decay\n",
        "        return ALPHA * np.sum(\n",
        "            [\n",
        "                np.linalg.norm(self.weights[str(i + 1)]) ** 2\n",
        "                for i in range(len(self.weights))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def accuracy(self, Y_true, Y_pred, data_size):\n",
        "        Y_true_label = []\n",
        "        Y_pred_label = []\n",
        "        ctr = 0\n",
        "        for i in range(data_size):\n",
        "            Y_true_label.append(np.argmax(Y_true[:, i]))\n",
        "            Y_pred_label.append(np.argmax(Y_pred[:, i]))\n",
        "            if Y_true_label[i] == Y_pred_label[i]:\n",
        "                ctr += 1\n",
        "        accuracy = ctr / data_size\n",
        "        return accuracy, Y_true_label, Y_pred_label\n",
        "\n",
        "    def Xavier_initializer(self, size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "        return np.random.normal(0, xavier_stddev, size=(out_dim, in_dim))\n",
        "\n",
        "    def random_initializer(self, size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        return np.random.normal(0, 1, size=(out_dim, in_dim))\n",
        "\n",
        "\n",
        "    def He_initializer(self,size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        He_stddev = np.sqrt(2 / (in_dim))\n",
        "        return np.random.normal(0, 1, size=(out_dim, in_dim)) * He_stddev\n",
        "\n",
        "\n",
        "    def initializeNeuralNet(self, layers):\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.initializer(size=[layers[l + 1], layers[l]])\n",
        "            b = np.zeros((layers[l + 1], 1))\n",
        "            weights[str(l + 1)] = W\n",
        "            biases[str(l + 1)] = b\n",
        "        return weights, biases\n",
        "\n",
        "    def forwardPropagate(self, X_train_batch, weights, biases):\n",
        "        \"\"\"\n",
        "        Returns the neural network given input data, weights, biases.\n",
        "        Arguments:\n",
        "                 : X - input matrix\n",
        "                 : Weights  - Weights matrix\n",
        "                 : biases - Bias vectors \n",
        "        \"\"\"\n",
        "        # Number of layers = length of weight matrix + 1\n",
        "        num_layers = len(weights) + 1\n",
        "        # A - Preactivations\n",
        "        # H - Activations\n",
        "        X = X_train_batch\n",
        "        H = {}\n",
        "        A = {}\n",
        "        H[\"0\"] = X\n",
        "        A[\"0\"] = X\n",
        "        for l in range(0, num_layers - 2):\n",
        "            if l == 0:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, X), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "            else:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, H[str(l)]), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "\n",
        "        # Here the last layer is not activated as it is a regression problem\n",
        "        W = weights[str(num_layers - 1)]\n",
        "        b = biases[str(num_layers - 1)]\n",
        "        A[str(num_layers - 1)] = np.add(np.matmul(W, H[str(num_layers - 2)]), b)\n",
        "        # Y = softmax(A[-1])\n",
        "        Y = softmax(A[str(num_layers - 1)])\n",
        "        H[str(num_layers - 1)] = Y\n",
        "        return Y, H, A\n",
        "\n",
        "    def backPropagate(\n",
        "        self, Y, H, A, Y_train_batch, weight_decay=0\n",
        "    ):\n",
        "\n",
        "        ALPHA = weight_decay\n",
        "        gradients_weights = []\n",
        "        gradients_biases = []\n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        # Gradient with respect to the output layer is absolutely fine.\n",
        "        if self.loss_function == \"CROSS\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = -(Y_train_batch - Y)\n",
        "        elif self.loss_function == \"MSE\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = np.multiply(\n",
        "                2 * (Y - Y_train_batch), np.multiply(Y, (1 - Y))\n",
        "            )\n",
        "\n",
        "        for l in range(num_layers - 2, -1, -1):\n",
        "\n",
        "            if ALPHA != 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = (\n",
        "                    np.outer(globals()[\"grad_a\" + str(l + 1)], H[str(l)])\n",
        "                    + ALPHA * self.weights[str(l + 1)]\n",
        "                )\n",
        "            elif ALPHA == 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = np.outer(\n",
        "                    globals()[\"grad_a\" + str(l + 1)], H[str(l)]\n",
        "                )\n",
        "            globals()[\"grad_b\" + str(l + 1)] = globals()[\"grad_a\" + str(l + 1)]\n",
        "            gradients_weights.append(globals()[\"grad_W\" + str(l + 1)])\n",
        "            gradients_biases.append(globals()[\"grad_b\" + str(l + 1)])\n",
        "            if l != 0:\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], self.der_activation(A[str(l)])\n",
        "                )\n",
        "            elif l == 0:\n",
        "\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], (A[str(l)])\n",
        "                )\n",
        "        return gradients_weights, gradients_biases\n",
        "\n",
        "\n",
        "    def predict(self,X,length_dataset):\n",
        "        Y_pred = []        \n",
        "        for i in range(length_dataset):\n",
        "\n",
        "            Y, H, A = self.forwardPropagate(\n",
        "                X[:, i].reshape(self.img_flattened_size, 1),\n",
        "                self.weights,\n",
        "                self.biases,\n",
        "            )\n",
        "\n",
        "            Y_pred.append(Y.reshape(self.num_classes,))\n",
        "        Y_pred = np.array(Y_pred).transpose()\n",
        "        return Y_pred\n",
        "\n",
        "    def sgd(self, epochs, length_dataset, learning_rate, weight_decay=0):\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            # perm = np.random.permutation(N)\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            deltaw = [\n",
        "                np.zeros((self.layers[l + 1], self.layers[l]))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "            deltab = [\n",
        "                np.zeros((self.layers[l + 1], 1))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "\n",
        "                Y, H, A = self.forwardPropagate(\n",
        "                    X_train[:, i].reshape(self.img_flattened_size, 1),\n",
        "                    self.weights,\n",
        "                    self.biases,\n",
        "                )\n",
        "                grad_weights, grad_biases = self.backPropagate(\n",
        "                    Y, H, A, Y_train[:, i].reshape(self.num_classes, 1)\n",
        "                )\n",
        "                deltaw = [\n",
        "                    grad_weights[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "                deltab = [\n",
        "                    grad_biases[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "\n",
        "                CE.append(\n",
        "                    self.crossEntropyLoss(\n",
        "                        self.Y_train[:, i].reshape(self.num_classes, 1), Y\n",
        "                    )\n",
        "                    + self.L2RegularisationLoss(weight_decay)\n",
        "                )\n",
        "\n",
        "                # print(num_points_seen)\n",
        "                self.weights = {\n",
        "                    str(i + 1): (self.weights[str(i + 1)] - learning_rate * deltaw[i])\n",
        "                    for i in range(len(self.weights))\n",
        "                }\n",
        "                self.biases = {\n",
        "                    str(i + 1): (self.biases[str(i + 1)] - learning_rate * deltab[i])\n",
        "                    for i in range(len(self.biases))\n",
        "                }\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "            \n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch, })\n",
        "        # data = [[epoch, loss[epoch]] for epoch in range(epochs)]\n",
        "        # table = wandb.Table(data=data, columns = [\"Epoch\", \"Loss\"])\n",
        "        # wandb.log({'loss':wandb.plot.line(table, \"Epoch\", \"Loss\", title=\"Loss vs Epoch Line Plot\")})\n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "      \n",
        "    def sgdMiniBatch(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        num_points_seen = 0\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "            \n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                    \n",
        "                    \n",
        "                    self.weights = {str(i+1):(self.weights[str(i+1)] - learning_rate*deltaw[i]/batch_size) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):(self.biases[str(i+1)] - learning_rate*deltab[i]) for i in range(len(self.biases))}\n",
        "                    \n",
        "                    #resetting gradient updates\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "\n",
        "    def mgd(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        num_points_seen = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "\n",
        "                    v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1) : (self.weights[str(i+1)] - v_w[i]) for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1): (self.biases[str(i+1)] - v_b[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    prev_v_w = v_w\n",
        "                    prev_v_b = v_b\n",
        "\n",
        "                    #resetting gradient updates\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "\n",
        "\n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        " \n",
        " \n",
        "    def stochasticNag(self,epochs,length_dataset, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []  \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            v_w = [GAMMA*prev_v_w[i] for i in range(0, len(self.layers)-1)]  \n",
        "            v_b = [GAMMA*prev_v_b[i] for i in range(0, len(self.layers)-1)]\n",
        "                        \n",
        "            for i in range(length_dataset):\n",
        "                winter = {str(i+1) : self.weights[str(i+1)] - v_w[i] for i in range(0, len(self.layers)-1)}\n",
        "                binter = {str(i+1) : self.biases[str(i+1)] - v_b[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), winter, binter) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                            \n",
        "                v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i] for i in range(num_layers - 1)]\n",
        "                v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i] for i in range(num_layers - 1)]\n",
        "        \n",
        "                self.weights = {str(i+1):self.weights[str(i+1)] - v_w[i] for i in range(len(self.weights))} \n",
        "                self.biases = {str(i+1):self.biases[str(i+1)] - v_b[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                prev_v_w = v_w\n",
        "                prev_v_b = v_b\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "    \n",
        "\n",
        "    def nag(self,epochs,length_dataset, batch_size,learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        num_points_seen = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []  \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            v_w = [GAMMA*prev_v_w[i] for i in range(0, len(self.layers)-1)]  \n",
        "            v_b = [GAMMA*prev_v_b[i] for i in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                winter = {str(i+1) : self.weights[str(i+1)] - v_w[i] for i in range(0, len(self.layers)-1)}\n",
        "                binter = {str(i+1) : self.biases[str(i+1)] - v_b[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), winter, binter) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "\n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:                            \n",
        "\n",
        "                    v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i]/batch_size for i in range(num_layers - 1)]\n",
        "        \n",
        "                    self.weights ={str(i+1):self.weights[str(i+1)]  - v_w[i] for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - v_b[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                    prev_v_w = v_w\n",
        "                    prev_v_b = v_b\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "    \n",
        "\n",
        "    \n",
        "    def rmsProp(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA = 1e-8, 0.9\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        num_points_seen = 0        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "                        \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "            \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "            \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))            \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                \n",
        "                    v_w = [BETA*v_w[i] + (1-BETA)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA*v_b[i] + (1-BETA)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "\n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)]  - deltaw[i]*(learning_rate/np.sqrt(v_w[i]+EPS)) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - deltab[i]*(learning_rate/np.sqrt(v_b[i]+EPS)) for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred  \n",
        "\n",
        "\n",
        "\n",
        "    def adam(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        \n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA1, BETA2 = 1e-8, 0.9, 0.99\n",
        "        \n",
        "        m_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "        \n",
        "        m_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]   \n",
        "        \n",
        "        num_points_seen = 0 \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "           \n",
        "            for i in range(length_dataset):\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))                 \n",
        "\n",
        "                num_points_seen += 1\n",
        "                ctr = 0\n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                    ctr += 1\n",
        "                \n",
        "                    m_w = [BETA1*m_w[i] + (1-BETA1)*deltaw[i] for i in range(num_layers - 1)]\n",
        "                    m_b = [BETA1*m_b[i] + (1-BETA1)*deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                    v_w = [BETA2*v_w[i] + (1-BETA2)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA2*v_b[i] + (1-BETA2)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    m_w_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    m_b_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                \n",
        "                    v_w_hat = [v_w[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    v_b_hat = [v_b[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/np.sqrt(v_w[i]+EPS))*m_w_hat[i] for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/np.sqrt(v_b[i]+EPS))*m_b_hat[i] for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "    \n",
        "    def nadam(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        GAMMA, EPS, BETA1, BETA2 = 0.9, 1e-8, 0.9, 0.99\n",
        "\n",
        "        m_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "\n",
        "        m_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)] \n",
        "\n",
        "        num_points_seen = 0 \n",
        "        \n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "\n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "\n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))   \n",
        "                num_points_seen += 1\n",
        "                \n",
        "                if num_points_seen % batch_size == 0:\n",
        "                    \n",
        "                    m_w = [BETA1*m_w[i] + (1-BETA1)*deltaw[i] for i in range(num_layers - 1)]\n",
        "                    m_b = [BETA1*m_b[i] + (1-BETA1)*deltab[i] for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    v_w = [BETA2*v_w[i] + (1-BETA2)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA2*v_b[i] + (1-BETA2)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    m_w_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    m_b_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                    \n",
        "                    v_w_hat = [v_w[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    v_b_hat = [v_b[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/(np.sqrt(v_w_hat[i])+EPS))*(BETA1*m_w_hat[i]+ (1-BETA1)*deltaw[i]) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/(np.sqrt(v_b_hat[i])+EPS))*(BETA1*m_b_hat[i] + (1-BETA1)*deltab[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "             \n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred  \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Z_3tVRXJA8Rv",
        "outputId": "6645e48d-045e-47a3-c4af-dfc79039b5c0"
      },
      "source": [
        "import wandb\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "(trainIn, trainOut), (testIn, testOut) = fashion_mnist.load_data()\n",
        "\n",
        "N_train_full = trainOut.shape[0]\n",
        "N_train = int(0.9*N_train_full)\n",
        "N_validation = int(0.1 * trainOut.shape[0])\n",
        "N_test = testOut.shape[0]\n",
        "\n",
        "\n",
        "idx  = np.random.choice(trainOut.shape[0], N_train_full, replace=False)\n",
        "idx2 = np.random.choice(testOut.shape[0], N_test, replace=False)\n",
        "\n",
        "trainInFull = trainIn[idx, :]\n",
        "trainOutFull = trainOut[idx]\n",
        "\n",
        "trainIn = trainInFull[:N_train,:]\n",
        "trainOut = trainOutFull[:N_train]\n",
        "\n",
        "validIn = trainInFull[N_train:, :]\n",
        "validOut = trainOutFull[N_train:]    \n",
        "\n",
        "testIn = testIn[idx2, :]\n",
        "testOut = testOut[idx2]\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"validationaccuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"max_epochs\": {\n",
        "            \"values\": [5, 10]\n",
        "        },\n",
        "\n",
        "        \"initializer\": {\n",
        "            \"values\": [\"RANDOM\", \"XAVIER\"]\n",
        "        },\n",
        "\n",
        "        \"num_layers\": {\n",
        "            \"values\": [2, 3, 4]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"num_hidden_neurons\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \n",
        "        \"activation\": {\n",
        "            \"values\": ['RELU', 'SIGMOID', 'TANH']\n",
        "        },\n",
        "        \n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001, 0.0001]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0, 0.0005,0.5]\n",
        "        },\n",
        "        \n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"SGD\", \"MGD\", \"NAG\", \"RMSPROP\", \"ADAM\",\"NADAM\"]\n",
        "        },\n",
        "                    \n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "        }\n",
        "        \n",
        "        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='CS6910-DeepLearningFundamentals-Assignment1', entity='rahulsundar')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: z26hfzd5\n",
            "Sweep URL: https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/z26hfzd5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyC4fPR8Cwgp"
      },
      "source": [
        "def train():    \n",
        "    config_defaults = dict(\n",
        "            max_epochs=5,\n",
        "            num_hidden_layers=3,\n",
        "            num_hidden_neurons=32,\n",
        "            weight_decay=0,\n",
        "            learning_rate=1e-3,\n",
        "            optimizer=\"ADAM\",\n",
        "            batch_size=16,\n",
        "            activation=\"TANH\",\n",
        "            initializer=\"XAVIER\",\n",
        "            loss=\"CROSS\",\n",
        "        )\n",
        "        \n",
        "    #wandb.init(project='CS6910-DeepLearningFundamentals-Assignment1', entity='rahulsundar', config = config_defaults)\n",
        "    wandb.init(config = config_defaults)\n",
        "\n",
        "\n",
        "    wandb.run.name = \"hl_\" + str(wandb.config.num_hidden_layers) + \"_hn_\" + str(wandb.config.num_hidden_neurons) + \"_opt_\" + wandb.config.optimizer + \"_act_\" + wandb.config.activation + \"_lr_\" + str(wandb.config.learning_rate) + \"_bs_\"+str(wandb.config.batch_size) + \"_init_\" + wandb.config.initializer + \"_ep_\"+ str(wandb.config.max_epochs)+ \"_l2_\" + str(wandb.config.weight_decay) \n",
        "    CONFIG = wandb.config\n",
        "\n",
        "\n",
        "    \n",
        "    #sweep_id = wandb.sweep(sweep_config)\n",
        "  \n",
        "\n",
        "    FFNN = FeedForwardNeuralNetwork(\n",
        "        num_hidden_layers=CONFIG.num_hidden_layers,\n",
        "        num_hidden_neurons=CONFIG.num_hidden_neurons,\n",
        "        X_train_raw=trainIn,\n",
        "        Y_train_raw=trainOut,\n",
        "        N_train = N_train,\n",
        "        X_val_raw = validIn,\n",
        "        Y_val_raw = validOut,\n",
        "        N_val = N_validation,\n",
        "        X_test_raw = testIn,\n",
        "        Y_test_raw = testOut,\n",
        "        N_test = N_test,\n",
        "        optimizer = CONFIG.optimizer,\n",
        "        batch_size = CONFIG.batch_size,\n",
        "        weight_decay = CONFIG.weight_decay,\n",
        "        learning_rate = CONFIG.learning_rate,\n",
        "        max_epochs = CONFIG.max_epochs,\n",
        "        activation = CONFIG.activation,\n",
        "        initializer = CONFIG.initializer,\n",
        "        loss = CONFIG.loss\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.learning_rate)\n",
        " "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "z4EENrgtDTnu",
        "outputId": "6dbcc7c8-2eaa-42a2-8555-9566d3355878"
      },
      "source": [
        "wandb.agent(sweep_id, train, count = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kd2pjd60 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: RELU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: RANDOM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: NADAM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulsundar\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">glowing-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/z26hfzd5\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/z26hfzd5</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/kd2pjd60\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/kd2pjd60</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210314_102521-kd2pjd60</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}